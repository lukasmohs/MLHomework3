# Homework 3

## Part 1: Optimization (5 points)

Optimization underlies the determination of parameter settings based on minimization of an objective. We derived this for maximum likelihood of linear regression and showed a regularized version. In this exercise, we will use gradient descent to learn the parameters for logistic regression. Then we will impose a regularizer to keep the parameters closer to 0 in attempts to reduce variance.

```{r, quiet=T, message=F}
require(dplyr)
# Generate some data - do not change
set.seed(12345)
data.size = 1000
get_data = function(n=data.size) {
  df = data.frame(x1 = rnorm(n),x2=4*runif(n)-2) %>%
    tbl_df() %>%
    mutate(y = (x1^2+x2^2)<1) %>%
    mutate(y = (function(.) {.[runif(data.size)<0.1] = 0; .})(y)) %>%
    mutate(y = (function(.) {.[runif(data.size)<0.1] = 1; .})(y))
  df
}

df = get_data(data.size)

# plot(x=df$x1,y=df$x2, col=df$y+1,
#      xlim=c(-3,3), ylim=c(-3,3), pch=18, xlab="x1", ylab="x2")
# or 
require(ggplot2)
ggplot(data = df, aes(x=x1,y=x2,color=y)) + geom_point() + coord_cartesian(xlim=c(-3,3), ylim=c(-3,3))

```

a. Perform logistic regression using glm or other package
```{r, quiet=T, message=F}
lr <- glm(formula = y  ~ .,
          family=binomial(link="logit"),data = df, control = list(maxit = 100))
```

b. Perform logistic regression using optim. Compare your parameters from optim to the glm parameters.
```{r, quiet=T, message=F}
linear_prediction = function(x1,x2, pars) {
  pars[1]+ pars[2]*x1 + pars[3]*x2
}
sq_loss = function(pars, y, x1,x2) {
  sum((y-linear_prediction(x1,x2,pars))^2) #toefte
}
opt <- optim(par=rep(0,3), fn=sq_loss, y=df$y, x1=df$x1, x2=df$x2)
opt$par
lr$coefficients
print("to compare the intercept:")
log(opt$par[1]/(1-opt$par[1]))
```

c. Perform logistic regression with L2 regularization
```{r, quiet=T, message=F}
sq_loss_l2_regularized = function(pars, y, x1, x2, lambda=100) {
  sq_loss(pars,y,x1,x2) + lambda*sum(pars[2]^2+pars[3]^2) # omit the intercept
}
optreg = optim(par=rep(0,3), sq_loss_l2_regularized, y=df$y, x1=df$x1, x2=df$x2)
optreg$par
```

d. Perform logistic regression using optim with quadratic terms
```{r, quiet=T, message=F}
linear_prediction = function(x1,x2, pars) {
  pars[1]+ pars[2]*x1 + pars[3]*x2
}
quadratic_term_prediction1 = function(x1,x2, pars) {
  (x1*pars[2] + x2*pars[3] + pars[1])^2
}
quadratic_term_prediction2 = function(x1,x2, pars) {
   pars[1] + x1* pars[2] + x2 * pars[3] + x1^2*pars[4] + x2^2 * pars[5] + x1*x2*pars[6]
}
sq_loss_quadratic = function(pars, y, x1, x2) {
  sum((y-quadratic_term_prediction2(x1,x2,pars))^2)
}
optquad <- optim(par=rep(0,6), fn=sq_loss_quadratic, y=df$y, x1=df$x1, x2=df$x2)
optquad$par

```

e. Perform logistic regression with L2 regularization with quadratic terms
```{r, quiet=T, message=F}
sq_loss_l2_regularized_quadratic = function(pars, y, x1, x2, lambda=100) {
  sq_loss_quadratic(pars,y,x1,x2) + lambda*sum(pars[2]^2+pars[3]^2)
}
optquadreg <- optim(par=rep(0,6), fn=sq_loss_l2_regularized_quadratic, y=df$y, x1=df$x1, x2=df$x2)
optquadreg$par
```

f. Create a learning curve for parts b, c, d, e. Interpret your learning curve.
```{r, quiet=T, message=F}
accuracies = data.frame(x = numeric(100), opt = numeric(100), optreg=numeric(100), optquad=numeric(100),optquadreg=numeric(100))
test = df[seq(nrow(df)/2,nrow(df),1),]
for (i in 1:100 ){
  optaccuracy = 0 
  optregaccuracy = 0 
  optquadaccuracy = 0 
  optquadregaccuracy = 0 
  train = df[seq(1,nrow(df)/2/100*i,1),]
  opt = optim(par=rep(0,3), fn=sq_loss, y=train$y, x1=train$x1, x2=train$x2)
  optreg = optim(par=rep(0,3), fn=sq_loss_l2_regularized, y=train$y, x1=train$x1, x2=train$x2)
  optquad = optim(par=rep(1,6), fn=sq_loss_quadratic, y=train$y, x1=train$x1, x2=train$x2)
  optquadreg = optim(par=rep(1,6), fn=sq_loss_l2_regularized_quadratic, y=train$y, x1=train$x1, x2=train$x2)
  for (c in 1:nrow(test)) {
    if((opt$par[1] + opt$par[2]*test$x1[c] + opt$par[3]*test$x2[c] >  mean(test$y)) == (test$y[c]==1 )) {
      optaccuracy = optaccuracy + 1
    }

    if((optreg$par[1] + optreg$par[2]*test$x1[c] + optreg$par[3]*test$x2[c] >  mean(test$y)) == (test$y[c]==1 )) {
      optregaccuracy = optregaccuracy + 1
    }
    if((optquadreg$par[1] + optquadreg$par[2]*test$x1[c] + optquadreg$par[3]*test$x2[c] + optquadreg$par[4]*test$x1[c]^2
          + optquadreg$par[5]*test$x2[c]^2 + optquadreg$par[6]*test$x2[c]*test$x1[c] >   mean(test$y)) == (test$y[c]==1 )) {
      optquadaccuracy = optquadaccuracy + 1
    }
    if((optquadreg$par[1] + optquadreg$par[2]*test$x1[c] + optquadreg$par[3]*test$x2[c] + optquadreg$par[4]*test$x1[c]^2
          + optquadreg$par[5]*test$x2[c]^2 + optquadreg$par[6]*test$x2[c]*test$x1[c] >   mean(test$y)) == (test$y[c]==1 )) {
      optquadregaccuracy = optquadregaccuracy + 1
    }
    accuracies$x[i] <- i
    accuracies$opt[i] <- optaccuracy/nrow(test)
    accuracies$optreg[i] <- optregaccuracy/nrow(test)
    accuracies$optquad[i] <- optquadaccuracy/nrow(test)
    accuracies$optquadreg[i] <- optquadregaccuracy/nrow(test)
  }
}

accuracies
ggplot(data = accuracies, aes(x))  + geom_line(aes(y = opt, color="#FF9999")) + geom_line(aes(y = optreg, color="#56B4E9")) + geom_line(aes(y = optquad,color="#F0E442")) + geom_line(aes(y = optquadreg,color="#CC79A7")) + coord_cartesian(xlim=c(0,100), ylim=c(0,1))


```


## Part 2: Machine Learning Methods: simulation of statin effect on heart attack occurrence (6 points)

Use the data generator ```source(homework3simulator.r)``` provided to conduct a simulation study on the effect of statin on MI (myocardial infarction/heart attack). The way the data generator works is it enumerates all possible states and associates each with a cell of the full joint distribution ```m``` of the form: ```c(feature vector, probability)```. Use ```get_data( nData , m )``` to sample ```nData``` examples.

Under the ground truth, what is the treatment effect of statins on MI?
```{r, quiet=T, message=F}
source("homework3simulator.r")
statin_sample <- get_data(1000,m)
ate = (sum(statin_sample$statin==0&statin_sample$mi==1)/sum(statin_sample$statin==0) - sum(statin_sample$statin==1&statin_sample$mi==1)/sum(statin_sample$statin==1)) * 100

print("The ATE (in %) evaluates to: ")
ate

```

Under the ground truth, what is the treatment effect of statins in patients with diabetes? Compare that to the treatment effect of statins in patients without diabetes.
**[response required]**

Under the ground truth model, what is the cross entropy error of the optimal prediction function (hint: A. is there irreducible error, i.e., given x, can you perfectly predict y? B. Given the best you can do, how well do you do in expectation)?
**[response required]**

Compare the performance of the following algorithms in prediction of MI: logistic regression, random forests, neural networks, and SVMs
Plot learning curves for 10, 30, 100, 300, 1000, 3000 training examples. Report any errors. Interpret your findings.
**[response required]**

## Part 3: Survival Analysis (4 points)
Conduct survival analysis. For this we will use the "mgus" data set from ```library(survival); mgus```. MGUS is "monogammopathy of unknown significance", and it is a medical condition identified in asymptomatic individuals who receive blood tests. In some cases it can be a precursor to the blood cancer called multiple myeloma. For more information on the study, see: http://www.mayoclinicproceedings.org/article/S0025-6196(12)60015-9/pdf

Use glm and glmnet to conduct survival analysis with and without regularization. Describe the task and plot a Kaplan Meier curve; conduct your analysis; interpret your findings.

**[responses required]**

**Nice, you're using some powerful tools in this homework!** You can now optimize and create your own objective function. You are using some of the strongest off-the-shelf methods in machine learning. And, you deployed what is arguably the most-commonly used algorithm in biostatistics.

### Submission instructions

Same as homework 2: please use git. **To submit the homework, email me a link to your git repository.** I should be able to type "git clone <url>" and have it download from a cloud service (github, bitbucket, etc). Note that if it is a private repository, you will need to permit me access to it (please provide access to jeremy.weiss@gmail.com).

Your git repository should contain at least two commits with useful comments on what has changed from the previous version(s). This should be visible when I type in ```git log```. The submission I will grade is at the HEAD revision unless specified otherwise in your email. Include your .Rmd file and your .html file solutions in the repository with your name and andrew ID.
