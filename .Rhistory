survivalData["pcdx"] = ifelse(survivalData["pcdx"]=="AM",1,0)
survivalData=survivalData %>% na.omit()
RCM = survivalData  %>%
(function(x) glmnet(x %>% select(-id,-futime,-pctime,-death) %>%
as.data.frame() %>% as.matrix(),
Surv(survivalData$futime, survivalData$death==1),
family="cox"))(.)
print("")
print("Coefficients of Regularized Cox Model: ")
coef(RCM, s=0.15)
library(survival)
survivalData = mgus %>% tbl_df()
KM = Surv(survivalData$futime, survivalData$death==1) %>%
(function(x) survfit(x ~ 1, data=survivalData))(.)
# KM curve
plot(KM, xlab= "Days",ylab="P(survive)", mark.time=T)
# Cox Model
CM = Surv(survivalData$futime, survivalData$death==1) %>%
(function(x) coxph(x ~  age  +  sex + dxyr + pcdx +alb + creat +  hgb + mspike , data=survivalData))(.)
"---------------------------"
"Coefficients of Cox Model: "
"---------------------------"
coef(CM)
# Reg Cox Model
library(glmnet)
survivalData["sex"] = ifelse(survivalData["sex"]=="male",1,0)
survivalData["pcdx"] = ifelse(survivalData["pcdx"]=="AM",1,0)
survivalData=survivalData %>% na.omit()
RCM = survivalData  %>%
(function(x) glmnet(x %>% select(-id,-futime,-pctime,-death) %>%
as.data.frame() %>% as.matrix(),
Surv(survivalData$futime, survivalData$death==1),
family="cox"))(.)
"--------------------------------------"
"Coefficients of Regularized Cox Model: "
"--------------------------------------"
coef(RCM, s=0.15)
library(survival)
survivalData = mgus %>% tbl_df()
KM = Surv(survivalData$futime, survivalData$death==1) %>%
(function(x) survfit(x ~ 1, data=survivalData))(.)
# KM curve
plot(KM, xlab= "Days",ylab="P(survive)", mark.time=T)
# Cox Model
CM = Surv(survivalData$futime, survivalData$death==1) %>%
(function(x) coxph(x ~  age  +  sex + dxyr + pcdx +alb + creat +  hgb + mspike , data=survivalData))(.)
"---------------------------"
"Coefficients of Cox Model: "
"---------------------------"
coef(CM)
# Reg Cox Model
library(glmnet)
survivalData["sex"] = ifelse(survivalData["sex"]=="male",1,0)
survivalData["pcdx"] = ifelse(survivalData["pcdx"]=="AM",1,0)
survivalData=survivalData %>% na.omit()
RCM = survivalData  %>%
(function(x) glmnet(x %>% select(-id,-futime,-pctime,-death) %>%
as.data.frame() %>% as.matrix(),
Surv(survivalData$futime, survivalData$death==1),
family="cox"))(.)
"--------------------------------------"
"Coefficients of Regularized Cox Model: "
"--------------------------------------"
coef(RCM, s=0.05)
plot(km, xlab= "Days",ylab="P(survive)", mark.time=T)
mean(survivalData$age)
accuracies = data.frame(x = numeric(100), opt = numeric(100), optreg=numeric(100), optquad=numeric(100),optquadreg=numeric(100))
test = df[seq(nrow(df)/2,nrow(df),1),]
require(dplyr)
# Generate some data - do not change
set.seed(12345)
data.size = 1000
get_data = function(n=data.size) {
df = data.frame(x1 = rnorm(n),x2=4*runif(n)-2) %>%
tbl_df() %>%
mutate(y = (x1^2+x2^2)<1) %>%
mutate(y = (function(.) {.[runif(data.size)<0.1] = 0; .})(y)) %>%
mutate(y = (function(.) {.[runif(data.size)<0.1] = 1; .})(y))
df
}
df = get_data(data.size)
# plot(x=df$x1,y=df$x2, col=df$y+1,
#      xlim=c(-3,3), ylim=c(-3,3), pch=18, xlab="x1", ylab="x2")
# or
require(ggplot2)
ggplot(data = df, aes(x=x1,y=x2,color=y)) + geom_point() + coord_cartesian(xlim=c(-3,3), ylim=c(-3,3))
lr <- glm(formula = y  ~ .,
family=binomial(link="logit"),data = df, control = list(maxit = 100))
linear_prediction = function(x1,x2, pars) {
pars[1]+ pars[2]*x1 + pars[3]*x2
}
sq_loss = function(pars, y, x1,x2) {
sum((y-linear_prediction(x1,x2,pars))^2) #toefte
}
opt <- optim(par=rep(0,3), fn=sq_loss, y=df$y, x1=df$x1, x2=df$x2)
opt$par
lr$coefficients
print("to compare the intercept:")
log(opt$par[1]/(1-opt$par[1]))
sq_loss_l2_regularized = function(pars, y, x1, x2, lambda=100) {
sq_loss(pars,y,x1,x2) + lambda*sum(pars[2]^2+pars[3]^2) # omit the intercept
}
optreg = optim(par=rep(0,3), sq_loss_l2_regularized, y=df$y, x1=df$x1, x2=df$x2)
optreg$par
sq_loss_l2_regularized_quadratic = function(pars, y, x1, x2, lambda=100) {
sq_loss_quadratic(pars,y,x1,x2) + lambda*sum(pars[2]^2+pars[3]^2)
}
optquadreg <- optim(par=rep(0,6), fn=sq_loss_l2_regularized_quadratic, y=df$y, x1=df$x1, x2=df$x2)
linear_prediction = function(x1,x2, pars) {
pars[1]+ pars[2]*x1 + pars[3]*x2
}
quadratic_term_prediction1 = function(x1,x2, pars) {
(x1*pars[2] + x2*pars[3] + pars[1])^2
}
quadratic_term_prediction2 = function(x1,x2, pars) {
pars[1] + x1* pars[2] + x2 * pars[3] + x1^2*pars[4] + x2^2 * pars[5] + x1*x2*pars[6]
}
sq_loss_quadratic = function(pars, y, x1, x2) {
sum((y-quadratic_term_prediction2(x1,x2,pars))^2)
}
optquad <- optim(par=rep(0,6), fn=sq_loss_quadratic, y=df$y, x1=df$x1, x2=df$x2)
optquad$par
sq_loss_l2_regularized_quadratic = function(pars, y, x1, x2, lambda=100) {
sq_loss_quadratic(pars,y,x1,x2) + lambda*sum(pars[2]^2+pars[3]^2)
}
optquadreg <- optim(par=rep(0,6), fn=sq_loss_l2_regularized_quadratic, y=df$y, x1=df$x1, x2=df$x2)
optquadreg$par
accuracies = data.frame(x = numeric(100), opt = numeric(100), optreg=numeric(100), optquad=numeric(100),optquadreg=numeric(100))
test = df[seq(nrow(df)/2,nrow(df),1),]
for (i in 1:100 ){
optaccuracy = 0
optregaccuracy = 0
optquadaccuracy = 0
optquadregaccuracy = 0
train = df[seq(1,nrow(df)/2/100*i,1),]
opt = optim(par=rep(0,3), fn=sq_loss, y=train$y, x1=train$x1, x2=train$x2)
optreg = optim(par=rep(0,3), fn=sq_loss_l2_regularized, y=train$y, x1=train$x1, x2=train$x2)
optquad = optim(par=rep(1,6), fn=sq_loss_quadratic, y=train$y, x1=train$x1, x2=train$x2)
optquadreg = optim(par=rep(1,6), fn=sq_loss_l2_regularized_quadratic, y=train$y, x1=train$x1, x2=train$x2)
for (c in 1:nrow(test)) {
if((opt$par[1] + opt$par[2]*test$x1[c] + opt$par[3]*test$x2[c] >  mean(test$y)) == (test$y[c]==1 )) {
optaccuracy = optaccuracy + 1
}
if((optreg$par[1] + optreg$par[2]*test$x1[c] + optreg$par[3]*test$x2[c] >  mean(test$y)) == (test$y[c]==1 )) {
optregaccuracy = optregaccuracy + 1
}
if((optquadreg$par[1] + optquadreg$par[2]*test$x1[c] + optquadreg$par[3]*test$x2[c] + optquadreg$par[4]*test$x1[c]^2
+ optquadreg$par[5]*test$x2[c]^2 + optquadreg$par[6]*test$x2[c]*test$x1[c] >   mean(test$y)) == (test$y[c]==1 )) {
optquadaccuracy = optquadaccuracy + 1
}
if((optquadreg$par[1] + optquadreg$par[2]*test$x1[c] + optquadreg$par[3]*test$x2[c] + optquadreg$par[4]*test$x1[c]^2
+ optquadreg$par[5]*test$x2[c]^2 + optquadreg$par[6]*test$x2[c]*test$x1[c] >   mean(test$y)) == (test$y[c]==1 )) {
optquadregaccuracy = optquadregaccuracy + 1
}
accuracies$x[i] <- i
accuracies$opt[i] <- optaccuracy/nrow(test)
accuracies$optreg[i] <- optregaccuracy/nrow(test)
accuracies$optquad[i] <- optquadaccuracy/nrow(test)
accuracies$optquadreg[i] <- optquadregaccuracy/nrow(test)
}
}
accuracies
ggplot(data = accuracies, aes(x)) + ylab("Accuracy") + xlab("Training Set Size (%)") +  + geom_line(aes(y = opt, color="#FF9999")) + geom_line(aes(y = optreg, color="#56B4E9")) + geom_line(aes(y = optquad,color="#F0E442")) + geom_line(aes(y = optquadreg,color="#CC79A7")) + coord_cartesian(xlim=c(0,100), ylim=c(0,1))
ggplot(data = accuracies, aes(x)) + ylab("Accuracy") + xlab("Training Set Size (%)")  + geom_line(aes(y = opt, color="Logistic Regression(LR)")) + geom_line(aes(y = optreg, color="LR with L2 Regularization")) + geom_line(aes(y = optquad,color="LR based on quadratic terms")) + geom_line(aes(y = optquadreg,color="LR based on quadratic terms L2 Regularization")) + coord_cartesian(xlim=c(0,100), ylim=c(0,1)) +scale_colour_manual("", breaks = c("Logistic Regression", "LR with L2 Regularization", "LR based on quadratic terms", "LR based on quadratic terms L2 Regularization"), values = c("red", "green", "blue","violet"))
ggplot(data = accuracies, aes(x)) + ylab("Accuracy") + xlab("Training Set Size (%)")  +  geom_line(aes(y = optreg, color="LR with L2 Regularization")) + geom_line(aes(y = opt, color="Logistic Regression(LR)")) + geom_line(aes(y = optquad,color="LR based on quadratic terms")) + geom_line(aes(y = optquadreg,color="LR based on quadratic terms L2 Regularization")) + coord_cartesian(xlim=c(0,100), ylim=c(0,1)) +scale_colour_manual("", breaks = c("Logistic Regression", "LR with L2 Regularization", "LR based on quadratic terms", "LR based on quadratic terms L2 Regularization"), values = c("red", "green", "blue","violet"))
60,
opt
ggplot(data = accuracies, aes(x)) + ylab("Accuracy") + xlab("Training Set Size (%)")  + geom_line(aes(y = opt, color="Logistic Regression(LR)")) + coord_cartesian(xlim=c(0,100), ylim=c(0,1)) + scale_colour_manual("", breaks = c("Logistic Regression", "LR with L2 Regularization", "LR based on quadratic terms", "LR based on quadratic terms L2 Regularization"), values = c("red", "green", "blue","violet"))
accuracies$opt
ggplot(data = accuracies, aes(x)) + ylab("Accuracy") + xlab("Training Set Size (%)")  + geom_line(aes(y = opt, color="Logistic Regression(LR)")) + geom_line(aes(y = optreg, color="LR with L2 Regularization")) + geom_line(aes(y = optquad,color="LR based on quadratic terms")) + geom_line(aes(y = optquadreg,color="LR based on quadratic terms L2 Regularization")) + coord_cartesian(xlim=c(0,100), ylim=c(0,1)) + scale_colour_manual("", breaks = c("Logistic Regression", "LR with L2 Regularization", "LR based on quadratic terms", "LR based on quadratic terms L2 Regularization"), values = c("red", "green", "blue","violet"))
ggplot(data = accuracies, aes(x)) + ylab("Accuracy") + xlab("Training Set Size (%)")  + geom_line(aes(y = opt, color="Logistic Regression(LR)")) + geom_line(aes(y = optreg, color="LR with L2 Regularization")) + geom_line(aes(y = optquad,color="LR based on quadratic terms")) + geom_line(aes(y = optquadreg,color="LR based on quadratic terms L2 Regularization")) + coord_cartesian(xlim=c(0,100), ylim=c(0,1)) + scale_colour_manual("", breaks = c("Logistic Regression(LR)", "LR with L2 Regularization", "LR based on quadratic terms", "LR based on quadratic terms L2 Regularization"), values = c("red", "green", "blue","violet"))
for (i in cases) {
library(neuralnet)
detach(package:neuralnet) #Killing the ROC prediction function...
library(ROCR)
statin_sample = get_data(i,m)
statin_test = get_data(i,m)
while(sum(statin_test$mi)==0 || sum(statin_sample$mi)==0) {
statin_sample = get_data(i,m)
statin_test = get_data(i,m)
}
##Logistic Regression
LR <- glm(formula = mi  ~ .,
family=binomial(link="logit"),data = statin_sample, control = list(maxit = 100))
LRPredictedProbabilities <- predict(LR, statin_test, type="response")
LRPred <- prediction(LRPredictedProbabilities, statin_test$mi)
LRROC <- performance(LRPred,"tpr","fpr")
##Random Forests
library(randomForest)
RF = randomForest(as.factor(mi==1) ~ ., statin_sample)
RFPredictedProbabilities = predict(RF,statin_test, type="prob")
RFPred <- prediction(RFPredictedProbabilities[,2], statin_test$mi)
RFROC <- performance(RFPred,"tpr","fpr")
##Neural Networks
library(neuralnet)
n = names(statin_sample)[-11]
rhs = paste(names(statin_sample[-11]), collapse = " + ")
formulaString = as.formula(paste0("mi~",rhs))
#nnSpectTrain = statin_sample %>% mutate_all((function(x) (x-mean(x))/sd(x)))
#nnSpectTest = statin_test %>% mutate_all(function(x) (x-mean(x))/sd(x))
NN = neuralnet(formula = formulaString,
data = statin_sample,#nnSpectTrain %>% mutate(mi=ifelse(mi>mean(mi),1,0)),
err.fct="ce",
act.fct="logistic",
linear.output = F,
hidden=c(1,2), # had to decrease since not converging
rep = 5,
stepmax=1e5)# takes a bit longer to converge :D
NNPrdictedProbabilites = compute(NN,statin_test %>% select(-mi))$net.result
detach(package:neuralnet) #Killing the ROC prediction function...
library(ROCR)
NNPred <- prediction(NNPrdictedProbabilites, statin_test$mi)
NNROC <- performance(NNPred,"tpr","fpr")
##SVM
library(e1071)
SVM = svm(mi ~ . , statin_sample)
SVMPredictedProbabilities = predict(SVM, statin_test, type="prob")
SVMPred <- prediction(SVMPredictedProbabilities, statin_test$mi)
SVMROC <- performance(SVMPred,"tpr","fpr")
##Plotting
plot_comparison(i)
}
for (i in cases) {
library(neuralnet)
detach(package:neuralnet) #Killing the ROC prediction function...
library(ROCR)
statin_sample = get_data(i,m)
statin_test = get_data(i,m)
while(sum(statin_test$mi)==0 || sum(statin_sample$mi)==0) {
statin_sample = get_data(i,m)
statin_test = get_data(i,m)
}
##Logistic Regression
LR <- glm(formula = mi  ~ .,
family=binomial(link="logit"),data = statin_sample, control = list(maxit = 100))
LRPredictedProbabilities <- predict(LR, statin_test, type="response")
LRPred <- prediction(LRPredictedProbabilities, statin_test$mi)
LRROC <- performance(LRPred,"tpr","fpr")
##Random Forests
library(randomForest)
RF = randomForest(as.factor(mi==1) ~ ., statin_sample)
RFPredictedProbabilities = predict(RF,statin_test, type="prob")
RFPred <- prediction(RFPredictedProbabilities[,2], statin_test$mi)
RFROC <- performance(RFPred,"tpr","fpr")
##Neural Networks
library(neuralnet)
n = names(statin_sample)[-11]
rhs = paste(names(statin_sample[-11]), collapse = " + ")
formulaString = as.formula(paste0("mi~",rhs))
#nnSpectTrain = statin_sample %>% mutate_all((function(x) (x-mean(x))/sd(x)))
#nnSpectTest = statin_test %>% mutate_all(function(x) (x-mean(x))/sd(x))
NN = neuralnet(formula = formulaString,
data = statin_sample,#nnSpectTrain %>% mutate(mi=ifelse(mi>mean(mi),1,0)),
err.fct="ce",
act.fct="logistic",
linear.output = F,
hidden=c(1,2), # had to decrease since not converging
rep = 5,
stepmax=1e5)# takes a bit longer to converge :D
NNPrdictedProbabilites = compute(NN,statin_test %>% select(-mi))$net.result
detach(package:neuralnet) #Killing the ROC prediction function...
library(ROCR)
NNPred <- prediction(NNPrdictedProbabilites, statin_test$mi)
NNROC <- performance(NNPred,"tpr","fpr")
##SVM
library(e1071)
SVM = svm(mi ~ . , statin_sample)
SVMPredictedProbabilities = predict(SVM, statin_test, type="prob")
SVMPred <- prediction(SVMPredictedProbabilities, statin_test$mi)
SVMROC <- performance(SVMPred,"tpr","fpr")
##Plotting
plot_comparison(i)
}
for (i in cases) {
library(neuralnet)
detach(package:neuralnet) #Killing the ROC prediction function...
library(ROCR)
statin_sample = get_data(i,m)
statin_test = get_data(i,m)
while(sum(statin_test$mi)==0 || sum(statin_sample$mi)==0) {
statin_sample = get_data(i,m)
statin_test = get_data(i,m)
}
##Logistic Regression
LR <- glm(formula = mi  ~ .,
family=binomial(link="logit"),data = statin_sample, control = list(maxit = 100))
LRPredictedProbabilities <- predict(LR, statin_test, type="response")
LRPred <- prediction(LRPredictedProbabilities, statin_test$mi)
LRROC <- performance(LRPred,"tpr","fpr")
##Random Forests
library(randomForest)
RF = randomForest(as.factor(mi==1) ~ ., statin_sample)
RFPredictedProbabilities = predict(RF,statin_test, type="prob")
RFPred <- prediction(RFPredictedProbabilities[,2], statin_test$mi)
RFROC <- performance(RFPred,"tpr","fpr")
##Neural Networks
library(neuralnet)
n = names(statin_sample)[-11]
rhs = paste(names(statin_sample[-11]), collapse = " + ")
formulaString = as.formula(paste0("mi~",rhs))
#nnSpectTrain = statin_sample %>% mutate_all((function(x) (x-mean(x))/sd(x)))
#nnSpectTest = statin_test %>% mutate_all(function(x) (x-mean(x))/sd(x))
NN = neuralnet(formula = formulaString,
data = statin_sample,#nnSpectTrain %>% mutate(mi=ifelse(mi>mean(mi),1,0)),
err.fct="ce",
act.fct="logistic",
linear.output = F,
hidden=c(1,2), # had to decrease since not converging
rep = 5,
stepmax=1e5)# takes a bit longer to converge :D
NNPrdictedProbabilites = compute(NN,statin_test %>% select(-mi))$net.result
detach(package:neuralnet) #Killing the ROC prediction function...
library(ROCR)
NNPred <- prediction(NNPrdictedProbabilites, statin_test$mi)
NNROC <- performance(NNPred,"tpr","fpr")
##SVM
library(e1071)
SVM = svm(mi ~ . , statin_sample)
SVMPredictedProbabilities = predict(SVM, statin_test, type="prob")
SVMPred <- prediction(SVMPredictedProbabilities, statin_test$mi)
SVMROC <- performance(SVMPred,"tpr","fpr")
##Plotting
plot_comparison(i)
}
cases = c(10,30,100,300,1000,3000)
plot_comparison = function(size){
plot(LRROC, col="blue")
plot(RFROC, add = TRUE, col="red")
plot(SVMROC, add = TRUE, col="green")
plot(NNROC, add = TRUE, col="violet")
title(main=cbind("ROC Comparison of different Classifiers, with n= ",size))
legend("bottomright",c("Logistic Regression","Random Forest","Support Vector Machine","Neural Network"),col=c("blue","red", "green","violet"), lwd=3, y.intersp = 1.0)
}
for (i in cases) {
library(neuralnet)
detach(package:neuralnet) #Killing the ROC prediction function...
library(ROCR)
statin_sample = get_data(i,m)
statin_test = get_data(i,m)
while(sum(statin_test$mi)==0 || sum(statin_sample$mi)==0) {
statin_sample = get_data(i,m)
statin_test = get_data(i,m)
}
##Logistic Regression
LR <- glm(formula = mi  ~ .,
family=binomial(link="logit"),data = statin_sample, control = list(maxit = 100))
LRPredictedProbabilities <- predict(LR, statin_test, type="response")
LRPred <- prediction(LRPredictedProbabilities, statin_test$mi)
LRROC <- performance(LRPred,"tpr","fpr")
##Random Forests
library(randomForest)
RF = randomForest(as.factor(mi==1) ~ ., statin_sample)
RFPredictedProbabilities = predict(RF,statin_test, type="prob")
RFPred <- prediction(RFPredictedProbabilities[,2], statin_test$mi)
RFROC <- performance(RFPred,"tpr","fpr")
##Neural Networks
library(neuralnet)
n = names(statin_sample)[-11]
rhs = paste(names(statin_sample[-11]), collapse = " + ")
formulaString = as.formula(paste0("mi~",rhs))
#nnSpectTrain = statin_sample %>% mutate_all((function(x) (x-mean(x))/sd(x)))
#nnSpectTest = statin_test %>% mutate_all(function(x) (x-mean(x))/sd(x))
NN = neuralnet(formula = formulaString,
data = statin_sample,#nnSpectTrain %>% mutate(mi=ifelse(mi>mean(mi),1,0)),
err.fct="ce",
act.fct="logistic",
linear.output = F,
hidden=c(1,2), # had to decrease since not converging
rep = 5,
stepmax=1e5)# takes a bit longer to converge :D
NNPrdictedProbabilites = compute(NN,statin_test %>% select(-mi))$net.result
detach(package:neuralnet) #Killing the ROC prediction function...
library(ROCR)
NNPred <- prediction(NNPrdictedProbabilites, statin_test$mi)
NNROC <- performance(NNPred,"tpr","fpr")
##SVM
library(e1071)
SVM = svm(mi ~ . , statin_sample)
SVMPredictedProbabilities = predict(SVM, statin_test, type="prob")
SVMPred <- prediction(SVMPredictedProbabilities, statin_test$mi)
SVMROC <- performance(SVMPred,"tpr","fpr")
##Plotting
plot_comparison(i)
}
require(dplyr)
set.seed(12345)
data.size = 1000
get_data = function(n=data.size) {
df = data.frame(x1 = rnorm(n),x2=4*runif(n)-2) %>%
tbl_df() %>%
mutate(y = (x1^2+x2^2)<1) %>%
mutate(y = (function(.) {.[runif(data.size)<0.1] = 0; .})(y)) %>%
mutate(y = (function(.) {.[runif(data.size)<0.1] = 1; .})(y))
df
}
df = get_data(data.size)
require(ggplot2)
require(ggplot2)
ggplot(data = df, aes(x=x1,y=x2,color=y)) + geom_point() + coord_cartesian(xlim=c(-3,3), ylim=c(-3,3))
linear_prediction = function(x1,x2, pars) {
pars[1]+ pars[2]*x1 + pars[3]*x2
}
sq_loss = function(pars, y, x1,x2) {
sum((y-linear_prediction(x1,x2,pars))^2) #toefte
}
opt <- optim(par=rep(0,3), fn=sq_loss, y=df$y, x1=df$x1, x2=df$x2)
opt$par
lr$coefficients
lr <- glm(formula = y  ~ .,
family=binomial(link="logit"),data = df, control = list(maxit = 100))
linear_prediction = function(x1,x2, pars) {
pars[1]+ pars[2]*x1 + pars[3]*x2
}
sq_loss = function(pars, y, x1,x2) {
sum((y-linear_prediction(x1,x2,pars))^2) #toefte
}
opt <- optim(par=rep(0,3), fn=sq_loss, y=df$y, x1=df$x1, x2=df$x2)
opt$par
lr$coefficients
print("to compare the intercept:")
log(opt$par[1]/(1-opt$par[1]))
sq_loss_l2_regularized = function(pars, y, x1, x2, lambda=100) {
sq_loss(pars,y,x1,x2) + lambda*sum(pars[2]^2+pars[3]^2) # omit the intercept
}
optreg = optim(par=rep(0,3), sq_loss_l2_regularized, y=df$y, x1=df$x1, x2=df$x2)
optreg$par
linear_prediction = function(x1,x2, pars) {
pars[1]+ pars[2]*x1 + pars[3]*x2
}
quadratic_term_prediction1 = function(x1,x2, pars) {
(x1*pars[2] + x2*pars[3] + pars[1])^2
}
quadratic_term_prediction2 = function(x1,x2, pars) {
pars[1] + x1* pars[2] + x2 * pars[3] + x1^2*pars[4] + x2^2 * pars[5] + x1*x2*pars[6]
}
sq_loss_quadratic = function(pars, y, x1, x2) {
sum((y-quadratic_term_prediction2(x1,x2,pars))^2)
}
optquad <- optim(par=rep(0,6), fn=sq_loss_quadratic, y=df$y, x1=df$x1, x2=df$x2)
optquad$par
sq_loss_l2_regularized_quadratic = function(pars, y, x1, x2, lambda=100) {
sq_loss_quadratic(pars,y,x1,x2) + lambda*sum(pars[2]^2+pars[3]^2)
}
optquadreg <- optim(par=rep(0,6), fn=sq_loss_l2_regularized_quadratic, y=df$y, x1=df$x1, x2=df$x2)
optquadreg$par
accuracies = data.frame(x = numeric(100), opt = numeric(100), optreg=numeric(100), optquad=numeric(100),optquadreg=numeric(100))
test = df[seq(nrow(df)/2,nrow(df),1),]
for (i in 1:100 ){
optaccuracy = 0
optregaccuracy = 0
optquadaccuracy = 0
optquadregaccuracy = 0
train = df[seq(1,nrow(df)/2/100*i,1),]
opt = optim(par=rep(0,3), fn=sq_loss, y=train$y, x1=train$x1, x2=train$x2)
optreg = optim(par=rep(0,3), fn=sq_loss_l2_regularized, y=train$y, x1=train$x1, x2=train$x2)
optquad = optim(par=rep(1,6), fn=sq_loss_quadratic, y=train$y, x1=train$x1, x2=train$x2)
optquadreg = optim(par=rep(1,6), fn=sq_loss_l2_regularized_quadratic, y=train$y, x1=train$x1, x2=train$x2)
for (c in 1:nrow(test)) {
if((opt$par[1] + opt$par[2]*test$x1[c] + opt$par[3]*test$x2[c] >  mean(test$y)) == (test$y[c]==1 )) {
optaccuracy = optaccuracy + 1
}
if((optreg$par[1] + optreg$par[2]*test$x1[c] + optreg$par[3]*test$x2[c] >  mean(test$y)) == (test$y[c]==1 )) {
optregaccuracy = optregaccuracy + 1
}
if((optquadreg$par[1] + optquadreg$par[2]*test$x1[c] + optquadreg$par[3]*test$x2[c] + optquadreg$par[4]*test$x1[c]^2
+ optquadreg$par[5]*test$x2[c]^2 + optquadreg$par[6]*test$x2[c]*test$x1[c] >   mean(test$y)) == (test$y[c]==1 )) {
optquadaccuracy = optquadaccuracy + 1
}
if((optquadreg$par[1] + optquadreg$par[2]*test$x1[c] + optquadreg$par[3]*test$x2[c] + optquadreg$par[4]*test$x1[c]^2
+ optquadreg$par[5]*test$x2[c]^2 + optquadreg$par[6]*test$x2[c]*test$x1[c] >   mean(test$y)) == (test$y[c]==1 )) {
optquadregaccuracy = optquadregaccuracy + 1
}
accuracies$x[i] <- i
accuracies$opt[i] <- optaccuracy/nrow(test)
accuracies$optreg[i] <- optregaccuracy/nrow(test)
accuracies$optquad[i] <- optquadaccuracy/nrow(test)
accuracies$optquadreg[i] <- optquadregaccuracy/nrow(test)
}
}
for (i in 1:100 ){
optaccuracy = 0
optregaccuracy = 0
optquadaccuracy = 0
optquadregaccuracy = 0
train = df[seq(1,nrow(df)/2/100*i,1),]
opt = optim(par=rep(0,3), fn=sq_loss, y=train$y, x1=train$x1, x2=train$x2)
optreg = optim(par=rep(0,3), fn=sq_loss_l2_regularized, y=train$y, x1=train$x1, x2=train$x2)
optquad = optim(par=rep(1,6), fn=sq_loss_quadratic, y=train$y, x1=train$x1, x2=train$x2)
optquadreg = optim(par=rep(1,6), fn=sq_loss_l2_regularized_quadratic, y=train$y, x1=train$x1, x2=train$x2)
for (c in 1:nrow(test)) {
if((opt$par[1] + opt$par[2]*test$x1[c] + opt$par[3]*test$x2[c] >  mean(test$y)) == (test$y[c]==1 )) {
optaccuracy = optaccuracy + 1
}
if((optreg$par[1] + optreg$par[2]*test$x1[c] + optreg$par[3]*test$x2[c] >  mean(test$y)) == (test$y[c]==1 )) {
optregaccuracy = optregaccuracy + 1
}
if((optquadreg$par[1] + optquadreg$par[2]*test$x1[c] + optquadreg$par[3]*test$x2[c] + optquadreg$par[4]*test$x1[c]^2
+ optquadreg$par[5]*test$x2[c]^2 + optquadreg$par[6]*test$x2[c]*test$x1[c] >   mean(test$y)) == (test$y[c]==1 )) {
optquadaccuracy = optquadaccuracy + 1
}
if((optquadreg$par[1] + optquadreg$par[2]*test$x1[c] + optquadreg$par[3]*test$x2[c] + optquadreg$par[4]*test$x1[c]^2
+ optquadreg$par[5]*test$x2[c]^2 + optquadreg$par[6]*test$x2[c]*test$x1[c] >   mean(test$y)) == (test$y[c]==1 )) {
optquadregaccuracy = optquadregaccuracy + 1
}
accuracies$x[i] <- i
accuracies$opt[i] <- optaccuracy/nrow(test)
accuracies$optreg[i] <- optregaccuracy/nrow(test)
accuracies$optquad[i] <- optquadaccuracy/nrow(test)
accuracies$optquadreg[i] <- optquadregaccuracy/nrow(test)
}
}
accuracies
